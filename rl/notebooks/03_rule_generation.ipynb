{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Generation Pipeline\n",
    "\n",
    "Generate intent rules from uDOM snapshots using HuggingFace models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# python-dotenv not installed, skip\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Now import from the rl package\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompt_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptLoader\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnapshot_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnapshotLoader\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema_validator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchemaValidator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/taste/rl/utils/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mUtility modules for RL pipeline\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptLoader, SnapshotLoader\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchemaValidator\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mPromptLoader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSnapshotLoader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSchemaValidator\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/taste/rl/utils/loaders/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mData loaders for RL pipeline\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompt_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptLoader, load_prompt\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnapshot_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnapshotLoader\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mPromptLoader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSnapshotLoader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mload_prompt\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/taste/rl/utils/loaders/prompt_loader.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Dict, List\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPromptLoader\u001b[39;00m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load system prompts from JSON database (file or API)\"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append(str(Path('../utils/loaders').resolve()))\n",
    "sys.path.append(str(Path('../utils/validators').resolve()))\n",
    "sys.path.append(str(Path('../evaluation').resolve()))\n",
    "\n",
    "from prompt_loader import PromptLoader\n",
    "from snapshot_loader import SnapshotLoader\n",
    "from schema_validator import SchemaValidator\n",
    "from quality_metrics import QualityMetrics\n",
    "\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "print(f\"HF_TOKEN: {'loaded' if HF_TOKEN else 'not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set HF_TOKEN directly here if not in environment\n",
    "# Uncomment and add your token:\n",
    "# os.environ['HF_TOKEN'] = 'your_huggingface_token_here'\n",
    "\n",
    "# Verify token is accessible\n",
    "if not HF_TOKEN:\n",
    "    print(\"[WARNING] HF_TOKEN not set. You can:\")\n",
    "    print(\"   1. Set it in the cell above\")\n",
    "    print(\"   2. Export in terminal: export HF_TOKEN='your_token'\")\n",
    "    print(\"   3. Create .env file in rl/ or taste/ directory\")\n",
    "else:\n",
    "    print(f\"[OK] HF_TOKEN ready (length: {len(HF_TOKEN)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('../config/training_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "MODEL_ID = config['model_config']['base_model']\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Alternatives: {config['model_config']['alternatives'][:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_loader = SnapshotLoader()\n",
    "stats = snapshot_loader.get_stats()\n",
    "print(f\"Storage: {stats.get('storage', 'unknown')}\")\n",
    "print(f\"Total snapshots: {stats.get('total_snapshots', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = snapshot_loader.load_recent(limit=10)\n",
    "print(f\"Loaded {len(snapshots)} snapshots\")\n",
    "for i, s in enumerate(snapshots[:3]):\n",
    "    sid = s.get('metadata', {}).get('snapshot_id', 'unknown')[:12]\n",
    "    print(f\"  {i+1}. {sid}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generator Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_loader = PromptLoader()\n",
    "generator_prompt = prompt_loader.load_prompt('generator')\n",
    "generator_text = generator_prompt['prompt_text']\n",
    "print(f\"Prompt: {generator_prompt.get('name')} v{generator_prompt.get('version')}\")\n",
    "print(f\"Length: {len(generator_text):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_snapshot(snapshot: dict) -> dict:\n",
    "    elements = snapshot.get('elements', [])\n",
    "    element_types = set(e.get('type', 'unknown') for e in elements)\n",
    "    property_types = set()\n",
    "    for e in elements:\n",
    "        property_types.update(e.get('properties', {}).keys())\n",
    "    \n",
    "    provenance = snapshot.get('observations', {}).get('provenance', {})\n",
    "    return {\n",
    "        'artifacts': {'before': snapshot, 'after': snapshot},\n",
    "        'platform_semantics': {\n",
    "            'element_types': list(element_types),\n",
    "            'property_types': list(property_types)[:10]\n",
    "        },\n",
    "        'platform_metadata': {\n",
    "            'platform': provenance.get('tool', 'figma'),\n",
    "            'extraction_method': provenance.get('extraction_method', 'plugin_api')\n",
    "        },\n",
    "        'generation_config': {'min_confidence': 0.6, 'max_rules_per_batch': 5}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshots:\n",
    "    generator_input = transform_snapshot(snapshots[0])\n",
    "    print(f\"Platform: {generator_input['platform_metadata']['platform']}\")\n",
    "    print(f\"Element types: {generator_input['platform_semantics']['element_types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Rules Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(generator_input: dict, trace_id: str = None) -> dict:\n",
    "    user_content = json.dumps(generator_input, indent=2)\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': generator_text},\n",
    "        {'role': 'user', 'content': f'Generate intent rules:\\n\\n{user_content}'}\n",
    "    ]\n",
    "    \n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        prompt = f\"System: {generator_text}\\n\\nUser: Generate intent rules:\\n\\n{user_content}\\n\\nAssistant:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        json_start = response.find('{')\n",
    "        json_end = response.rfind('}') + 1\n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            result = json.loads(response[json_start:json_end])\n",
    "        else:\n",
    "            result = {'intent_rules': [], 'raw_response': response}\n",
    "    except json.JSONDecodeError:\n",
    "        result = {'intent_rules': [], 'raw_response': response}\n",
    "    \n",
    "    result['trace_id'] = trace_id or f\"trace_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    result['generated_at'] = datetime.now().isoformat()\n",
    "    result['model'] = MODEL_ID\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshots:\n",
    "    print(\"Generating rules...\")\n",
    "    rules_output = generate_rules(generator_input, 'test_trace')\n",
    "    print(f\"Generated {len(rules_output.get('intent_rules', []))} rules\")\n",
    "    print(f\"Model: {rules_output.get('model')}\")\n",
    "    for i, rule in enumerate(rules_output.get('intent_rules', [])[:3]):\n",
    "        print(f\"  {i+1}. {rule.get('description', 'N/A')[:60]}...\")\n",
    "else:\n",
    "    rules_output = None\n",
    "    print(\"No snapshots available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(snapshots: list, batch_size: int = 3, output_path: Path = None) -> list:\n",
    "    \"\"\"Process multiple snapshots and generate rules\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = Path('../data/generated_rules/rules.jsonl')\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_outputs = []\n",
    "    total_rules = 0\n",
    "    \n",
    "    for i, snapshot in enumerate(snapshots[:batch_size]):\n",
    "        sid = snapshot.get('metadata', {}).get('snapshot_id', f'snapshot_{i}')\n",
    "        print(f\"[{i+1}/{batch_size}] Processing {sid[:12]}...\")\n",
    "        try:\n",
    "            gen_input = transform_snapshot(snapshot)\n",
    "            result = generate_rules(gen_input, sid)\n",
    "            num_rules = len(result.get('intent_rules', []))\n",
    "            all_outputs.append(result)\n",
    "            total_rules += num_rules\n",
    "            \n",
    "            with open(output_path, 'a') as f:\n",
    "                f.write(json.dumps(result) + '\\n')\n",
    "            print(f\"  Generated {num_rules} rules\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(all_outputs)} snapshots\")\n",
    "    print(f\"Total rules generated: {total_rules}\")\n",
    "    return all_outputs\n",
    "\n",
    "# Uncomment to run batch processing:\n",
    "# batch_outputs = process_batch(snapshots, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_validator = SchemaValidator()\n",
    "quality_metrics = QualityMetrics()\n",
    "\n",
    "if rules_output and rules_output.get('intent_rules'):\n",
    "    rules = rules_output['intent_rules']\n",
    "    valid_count = sum(1 for r in rules if schema_validator.validate_rule(r)[0])\n",
    "    print(f\"Validation: {valid_count}/{len(rules)} valid\")\n",
    "    \n",
    "    metrics = quality_metrics.compute_rule_quality(rules)\n",
    "    print(f\"Avg confidence: {metrics.avg_confidence:.2f}\")\n",
    "    print(f\"Quality score: {metrics.quality_score:.2f}\")\n",
    "else:\n",
    "    print(\"No rules to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_path = Path('../data/generated_rules/rules.jsonl')\n",
    "if rules_path.exists():\n",
    "    all_rules = []\n",
    "    with open(rules_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data = json.loads(line)\n",
    "                all_rules.extend(data.get('intent_rules', []))\n",
    "    print(f\"Loaded {len(all_rules)} rules from file\")\n",
    "else:\n",
    "    print(f\"No rules file at {rules_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- 04_dataset_building.ipynb: Generate preference pairs\n",
    "- 05_dataset_analysis.ipynb: Analyze dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
